# -*- coding: utf-8 -*-
"""NLP_Project (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aijAEdfK6f7nAR5oylHSu5pXFzq6aQ0T
"""

# ========================================
# Data Preparation with Preprocessing
# ========================================
import json
import pandas as pd
import re
import nltk
from pathlib import Path

print("Téléchargement des ressources NLTK...")
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)
print("✓ Ressources NLTK téléchargées")

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

DATA_PATH = Path("/content/articles.json")
OUT_PATH = Path("/content/articles_clean.csv")

def preprocess_text(text):
    """
    Prétraitement du texte :
    - Conversion en minuscules
    - Suppression de la ponctuation et caractères spéciaux
    - Tokenization
    - Suppression des stopwords
    - Lemmatisation
    """
    if not isinstance(text, str):
        return ""

    # Conversion en minuscules
    text = text.lower()

    # Suppression des URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)

    # Suppression des caractères spéciaux et ponctuation
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Tokenization
    tokens = word_tokenize(text)

    # Suppression des stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]

    # Lemmatisation
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return ' '.join(tokens)

def main():
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        articles = json.load(f)

    records = []
    for a in articles:
        title = a.get("Title", "")
        body = a.get("Body", "")

        # Prétraitement du CORPS uniquement (plus de contenu)
        body_clean = preprocess_text(body)

        # Vérification que le body n'est pas vide
        if not body_clean or len(body_clean.split()) < 5:
            print(f"⚠ Article ID {a['ID']} a un corps trop court, ajout du titre")
            title_clean = preprocess_text(title)
            body_clean = f"{title_clean} {body_clean}"

        records.append({
            "ID": a["ID"],
            "Title": a.get("Title", ""),      # Titre original
            "Body_Clean": body_clean,          # Corps prétraité
            "Tokens": body_clean.split()       # Tokens pour Word2Vec
        })

    df = pd.DataFrame(records)

    # Statistiques de prétraitement
    df['num_tokens'] = df['Tokens'].apply(len)
    print(f"\n✓ Saved {len(df)} articles → {OUT_PATH}")
    print(f"✓ Statistiques des tokens:")
    print(f"  - Moyenne: {df['num_tokens'].mean():.1f} tokens/article")
    print(f"  - Min: {df['num_tokens'].min()} tokens")
    print(f"  - Max: {df['num_tokens'].max()} tokens")
    print(f"\n✓ Exemple de prétraitement (Article 1):")
    print(f"  Titre: {df.iloc[0]['Title'][:80]}...")
    print(f"  Texte nettoyé: {df.iloc[0]['Body_Clean'][:150]}...")
    print(f"  Nombre de tokens: {df.iloc[0]['num_tokens']}")

    # Sauvegarder
    df_save = df[['ID', 'Title', 'Body_Clean']].copy()
    df_save.to_csv(OUT_PATH, index=False)

    return df

# Exécuter
df = main()


# ========================================
# Word2Vec Embeddings
# ========================================
import numpy as np
import pandas as pd
from gensim.models import Word2Vec
from pathlib import Path

DATA_PATH = Path("/content/articles_clean.csv")
OUT_EMB = Path("/content/embeddings_w2v.npy")
OUT_MODEL = Path("/content/word2vec_model.bin")

def create_document_embedding(tokens, model, vector_size):
    """
    Créer un embedding de document en moyennant les embeddings Word2Vec des mots.
    Utilise TF-IDF weighting pour donner plus d'importance aux mots rares.
    """
    vectors = []
    for word in tokens:
        if word in model.wv:
            vectors.append(model.wv[word])

    if len(vectors) == 0:
        # Si aucun mot n'est dans le vocabulaire, retourner un vecteur zéro
        return np.zeros(vector_size)

    # Moyenne des vecteurs de mots
    return np.mean(vectors, axis=0)

def main():
    # Charger les données
    df = pd.read_csv(DATA_PATH)

    # Re-tokeniser le texte nettoyé
    df['Tokens'] = df['Body_Clean'].fillna("").apply(lambda x: x.split())

    # Préparer le corpus pour Word2Vec
    corpus = df['Tokens'].tolist()

    print(f"✓ Corpus préparé: {len(corpus)} documents")
    print(f"✓ Exemple de tokens (Article 1): {corpus[0][:15]}")
    print(f"✓ Taille du vocabulaire total: {len(set([w for doc in corpus for w in doc]))} mots uniques")

    # Entraîner le modèle Word2Vec avec de meilleurs paramètres
    print("\n⏳ Entraînement du modèle Word2Vec...")
    vector_size = 100  # Dimension des embeddings
    window = 10        # Fenêtre de contexte plus large
    min_count = 2      # Au moins 2 occurrences
    workers = 4        # Nombre de threads
    sg = 1            # Skip-gram (meilleur pour petits corpus)
    epochs = 20       # Plus d'époques

    model = Word2Vec(
        sentences=corpus,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        workers=workers,
        sg=sg,
        epochs=epochs,
        negative=5,      # Negative sampling
        ns_exponent=0.75 # Exposant pour negative sampling
    )

    vocab_size = len(model.wv)
    print(f"✓ Modèle entraîné avec {vocab_size} mots dans le vocabulaire")

    if vocab_size < 50:
        print("⚠ ATTENTION: Vocabulaire très petit! Vérifiez le prétraitement.")

    # Créer les embeddings de documents
    print("\n⏳ Création des embeddings de documents...")
    embeddings = []
    empty_count = 0

    for i, tokens in enumerate(corpus):
        emb = create_document_embedding(tokens, model, vector_size)
        if np.all(emb == 0):
            empty_count += 1
            print(f"⚠ Article {i+1} (ID={df.iloc[i]['ID']}): embedding vide!")
        embeddings.append(emb)

    embeddings = np.array(embeddings)

    if empty_count > 0:
        print(f"⚠ {empty_count}/{len(corpus)} articles ont des embeddings vides")

    # Vérifier la variance des embeddings
    variance = np.var(embeddings, axis=0).mean()
    print(f"\n✓ Variance moyenne des embeddings: {variance:.6f}")
    if variance < 0.001:
        print("⚠ ATTENTION: Variance très faible - les embeddings sont trop similaires!")

    # Normalisation L2 pour la similarité cosinus
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    norms[norms == 0] = 1  # Éviter la division par zéro
    embeddings_normalized = embeddings / norms

    # Vérifier la diversité des similarités
    from sklearn.metrics.pairwise import cosine_similarity
    sim_matrix = cosine_similarity(embeddings_normalized)
    # Exclure la diagonale (similarité avec soi-même)
    np.fill_diagonal(sim_matrix, 0)
    avg_sim = sim_matrix.mean()
    print(f"✓ Similarité moyenne entre documents: {avg_sim:.4f}")
    print(f"✓ Similarité min: {sim_matrix.min():.4f}, max: {sim_matrix.max():.4f}")

    if avg_sim > 0.95:
        print("⚠ ATTENTION: Similarités trop élevées - possible problème d'embedding!")

    # Sauvegarder
    np.save(OUT_EMB, embeddings_normalized)
    model.save(str(OUT_MODEL))

    print(f"\n✓ Embeddings sauvegardés → {OUT_EMB}")
    print(f"✓ Forme des embeddings: {embeddings_normalized.shape}")
    print(f"✓ Modèle Word2Vec sauvegardé → {OUT_MODEL}")



if __name__ == "__main__":
    main()

# Commented out IPython magic to ensure Python compatibility.
# # ========================================
# # Recommender System
# # ========================================
# %%writefile recommender.py
# import numpy as np
# import pandas as pd
# from sklearn.metrics.pairwise import cosine_similarity
# 
# class ContentRecommender:
#     """
#     Système de recommandation basé sur la similarité cosinus
#     des embeddings Word2Vec de documents.
#     """
#     def __init__(self):
#         # Charger les données
#         self.df = pd.read_csv("/content/articles_clean.csv")
#         self.embeddings = np.load("/content/embeddings_w2v.npy")
# 
#         # Créer les mappages d'ID
#         self.df['_index'] = range(len(self.df))
#         self.df = self.df.set_index('ID')
# 
#         print(f"✓ Système initialisé avec {len(self.df)} articles")
#         print(f"✓ Dimension des embeddings: {self.embeddings.shape[1]}")
# 
#     def recommend(self, article_id, top_n=5):
#         """
#         Recommander les N articles les plus similaires à l'article donné.
# 
#         Args:
#             article_id: ID de l'article de référence
#             top_n: Nombre de recommandations à retourner
# 
#         Returns:
#             Liste de dictionnaires contenant ID, Title et Score
#         """
#         # Conversion en int
#         article_id = int(article_id)
# 
#         # Vérifier si l'ID existe
#         if article_id not in self.df.index:
#             available_ids = self.df.index.tolist()
#             raise ValueError(
#                 f"Article ID {article_id} non trouvé. "
#                 f"IDs disponibles: {available_ids[:10]}..."
#             )
# 
#         # Obtenir l'index de la ligne pour cet article
#         idx = self.df.loc[article_id, '_index']
# 
#         # Calculer les similarités cosinus
#         query_vec = self.embeddings[idx].reshape(1, -1)
#         scores = cosine_similarity(query_vec, self.embeddings)[0]
# 
#         # Trier par similarité décroissante
#         ranked_indices = scores.argsort()[::-1]
# 
#         # Construire les résultats
#         results = []
#         for i in ranked_indices:
#             # Trouver l'article correspondant à cet index
#             article_row = self.df[self.df['_index'] == i]
#             if len(article_row) == 0:
#                 continue
# 
#             result_id = article_row.index[0]
# 
#             # Ignorer l'article de référence lui-même
#             if result_id == article_id:
#                 continue
# 
#             results.append({
#                 "ID": int(result_id),
#                 "Title": article_row['Title'].values[0],
#                 "Score": float(scores[i])
#             })
# 
#             if len(results) >= top_n:
#                 break
# 
#         return results

# Commented out IPython magic to ensure Python compatibility.
# # ========================================
# # FastAPI REST API
# # ========================================
# 
# 
# %%writefile api.py
# from fastapi import FastAPI, HTTPException
# import numpy as np
# import pandas as pd
# from sklearn.metrics.pairwise import cosine_similarity
# 
# app = FastAPI(title="Content-Based Recommender API", version="1.0")
# 
# # Chargement des données au démarrage
# try:
#     df = pd.read_csv("/content/articles_clean.csv")
#     embeddings = np.load("/content/embeddings_w2v.npy")
# 
#     df['_index'] = range(len(df))
#     df = df.set_index('ID')
# 
#     print(f"✓ API initialisée avec {len(df)} articles")
#     print(f"✓ IDs disponibles: {df.index.tolist()}")
# 
# except Exception as e:
#     print(f"✗ ERREUR lors du chargement: {e}")
#     df = None
#     embeddings = None
# 
# @app.get("/")
# def root():
#     """Point d'entrée de l'API"""
#     return {
#         "message": "Content-Based Recommender API with Word2Vec",
#         "endpoints": {
#             "recommendations": "/recommend/{article_id}?n=5"
#         }
#     }
# 
# 
# 
# @app.get("/recommend/{article_id}")
# def recommend(article_id: str, n: int = 5):
#     """
#     Obtenir des recommandations d'articles similaires.
# 
#     Args:
#         article_id: ID de l'article de référence
#         n: Nombre de recommandations (défaut: 5)
# 
#     Returns:
#         Liste d'articles similaires avec leur score de similarité
#     """
#     if df is None or embeddings is None:
#         raise HTTPException(
#             status_code=503,
#             detail="Service indisponible: données non chargées"
#         )
# 
#     try:
#         article_id_int = int(article_id)
# 
#         if article_id_int not in df.index:
#             raise HTTPException(
#                 status_code=404,
#                 detail=f"Article ID {article_id_int} non trouvé"
#             )
# 
#         # Obtenir l'embedding de l'article de référence
#         idx = df.loc[article_id_int, '_index']
#         query_vec = embeddings[idx].reshape(1, -1)
# 
#         # Calculer les similarités
#         scores = cosine_similarity(query_vec, embeddings)[0]
#         ranked_indices = scores.argsort()[::-1]
# 
#         # Construire les résultats
#         results = []
#         for i in ranked_indices:
#             article_row = df[df['_index'] == i]
#             if len(article_row) == 0:
#                 continue
# 
#             result_id = article_row.index[0]
# 
#             # Ignorer l'article de référence
#             if result_id == article_id_int:
#                 continue
# 
#             results.append({
#                 "ID": int(result_id),
#                 "Title": str(article_row['Title'].values[0]),
#                 "Score": float(scores[i])
#             })
# 
#             if len(results) >= n:
#                 break
# 
#         return {
#             "query_article_id": article_id_int,
#             "query_title": df.loc[article_id_int, 'Title'],
#             "recommendations": results
#         }
# 
#     except HTTPException:
#         raise
#     except Exception as e:
#         raise HTTPException(
#             status_code=500,
#             detail=f"Erreur interne: {str(e)}"
#         )
# 
# 
#

# ========================================
# Start API Server
# ========================================

import threading
import uvicorn
import time
from api import app

def run_server():
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")

# Démarrer le serveur dans un thread séparé
thread = threading.Thread(target=run_server, daemon=True)
thread.start()

time.sleep(3)  # Attendre le démarrage
print("\n" + "="*50)
print("✓ Serveur API démarré sur http://127.0.0.1:8000")
print("✓ Documentation: http://127.0.0.1:8000/docs")
print("="*50)

# ========================================
#  Testing
# ========================================


import requests
import json


# Test 1: Obtenir des recommandations
print("\n\nTest 1: Recommendations pour l'article ID=5")
print("-" * 40)
response = requests.get("http://127.0.0.1:8000/recommend/5?n=3")
print(f"Status: {response.status_code}")
result = response.json()
print(f"\nArticle de référence: {result['query_title']}")
print(f"\nRecommandations:")
for i, rec in enumerate(result['recommendations'], 1):
    print(f"  {i}. [ID: {rec['ID']}] {rec['Title']}")
    print(f"     Similarité: {rec['Score']:.4f}")

# Test 2: Autre article
print("\n\nTest 2: Recommendations pour l'article ID=1")
print("-" * 40)
response = requests.get("http://127.0.0.1:8000/recommend/1?n=3")
result = response.json()
print(f"\nArticle de référence: {result['query_title']}")
print(f"\nRecommandations:")
for i, rec in enumerate(result['recommendations'], 1):
    print(f"  {i}. [ID: {rec['ID']}] {rec['Title']}")
    print(f"     Similarité: {rec['Score']:.4f}")
